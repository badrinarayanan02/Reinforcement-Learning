{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNX/oD2bNwhEXnZED6MH0LY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrinarayanan02/Reinforcement-Learning/blob/main/2348507_RLLab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Process"
      ],
      "metadata": {
        "id": "HiHfQKi7r9fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MDP** is a mathematical framework that is used to make decisions on uncertainity. It maximizes the expected future reward. We need to define states, actions, transitions, rewards, discount."
      ],
      "metadata": {
        "id": "7EJR5RJqsAfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing policy evaluation and improvement for a given MDP"
      ],
      "metadata": {
        "id": "y0KPGDRQre-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PrUyi5CZrbgP"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_states = 4\n",
        "num_actions = 2\n",
        "\n",
        "P = {\n",
        "    0: {\n",
        "        0: [(1.0, 1, 10)],\n",
        "        1: [(1.0, 0, 0)]\n",
        "    },\n",
        "    1: {\n",
        "        0: [(1.0, 2, -10)],\n",
        "        1: [(1.0, 0, 10)]\n",
        "    },\n",
        "    2: {\n",
        "        0: [(1.0, 3, 10)],\n",
        "        1: [(1.0, 1, -10)]\n",
        "    },\n",
        "    3: {\n",
        "        0: [(1.0, 3, 0)],\n",
        "        1: [(1.0, 3, 0)]\n",
        "    }\n",
        "}\n",
        "\n",
        "gamma = 0.9  # Discount factor\n",
        "theta = 1e-4  # Convergence threshold\n",
        "\n",
        "policy = np.ones([num_states, num_actions]) / num_actions"
      ],
      "metadata": {
        "id": "11ukBucut6p3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "CaPo5_qwzbyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented states and actions in MDP process with specified transitions and rewards and initializes the parameters and an initial policy for policy iteration. Specified parameters are Gamma and Theta. Gamma is a discount factor, the value of 0.9 means the future rewards are discounted by 10% for each time setup. Theta value of 1e-4 ensures that the process will stop when the value updates are less than this threshold."
      ],
      "metadata": {
        "id": "5ngBXp9qzc_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Evaluation"
      ],
      "metadata": {
        "id": "23Lnqwaq7moW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, P, gamma, theta):\n",
        "    V = np.zeros(num_states)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            v = 0\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                for prob, next_state, reward in P[s][a]:\n",
        "                    v += action_prob * prob * (reward + gamma * V[next_state])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "            V[s] = v\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V"
      ],
      "metadata": {
        "id": "_Wg8Rpdiv6aP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "ItKUYTWdzdAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The policy_evaluation function helps assess how good a policy is, setting the stage for policy improvement to make the policy even better, ultimately leading to an optimal policy.\n",
        "\n",
        "We can take a example of grid world where an agent moves between states and earns rewards. The policy_evaluation function calculates the expected value for each position in the grid, telling the agent how beneficial each position is when following a specific policy. This helps the agent learn which paths are better to take to maximize rewards over time."
      ],
      "metadata": {
        "id": "06EaQBjO2nM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Improvement"
      ],
      "metadata": {
        "id": "zB5R1WXW7pK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_improvement(V, P, gamma):\n",
        "    policy_stable = True\n",
        "    for s in range(num_states):\n",
        "        old_action = np.argmax(policy[s])\n",
        "\n",
        "        # Finding the best action based on current value function\n",
        "        action_values = np.zeros(num_actions)\n",
        "        for a in range(num_actions):\n",
        "            for prob, next_state, reward in P[s][a]:\n",
        "                action_values[a] += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "        new_action = np.argmax(action_values)\n",
        "        if old_action != new_action:\n",
        "            policy_stable = False\n",
        "        policy[s] = np.eye(num_actions)[new_action]\n",
        "    return policy, policy_stable"
      ],
      "metadata": {
        "id": "A8IRqYqwwPkh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "huhgGnGy1Osk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the agent is navigating a simple grid with rewards scattered throughout. After evaluating a policy, it might find that moving up is beneficial in a certain state due to a nearby reward. The policy improvement function will then adjust the policy to prioritize moving up in that state.\n",
        "\n",
        "If each state has now been assigned actions that maximize rewards based on the latest state values, the policy is improved. Through multiple cycles, this process gradually optimizes the policy until it becomes the optimal policy for maximizing rewards in the environment."
      ],
      "metadata": {
        "id": "MQ4vN9dJ4Z-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy Iteration"
      ],
      "metadata": {
        "id": "1LsCkqk67sX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(P, gamma, theta):\n",
        "    global policy\n",
        "    while True:\n",
        "        # Policy Evaluation\n",
        "        V = policy_evaluation(policy, P, gamma, theta)\n",
        "\n",
        "        # Policy Improvement\n",
        "        policy, policy_stable = policy_improvement(V, P, gamma)\n",
        "\n",
        "        if policy_stable:\n",
        "            return policy, V"
      ],
      "metadata": {
        "id": "jrABBnK8xD3S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "r2jpSJll51V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The policy_iteration function alternates between evaluating the policy and improving the policy until it finds the optimal policy. The returned policy is the optimal policy, and V is the optimal state-value function, which represents the maximum expected rewards for each state under the optimal policy. This iterative approach makes policy iteration a fundamental method for solving MDPs in reinforcement learning."
      ],
      "metadata": {
        "id": "RUyjUfh552vx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimal Policy and Optimal Value"
      ],
      "metadata": {
        "id": "8B0Igghu7wf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy, optimal_value = policy_iteration(P, gamma, theta)\n",
        "\n",
        "print(\"Optimal Policy (state-action probabilities):\")\n",
        "print(optimal_policy)\n",
        "print(\"\\nOptimal State-Value Function:\")\n",
        "print(optimal_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPag0TOFxiHA",
        "outputId": "5ef9ca40-5f32-4dcd-f703-ffa6c142b5c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy (state-action probabilities):\n",
            "[[1. 0.]\n",
            " [0. 1.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n",
            "\n",
            "Optimal State-Value Function:\n",
            "[99.99964119 99.99967708 79.99970937  0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "S6hx6GFZ6K-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State 0 has a value of approximately 99.9996\n",
        "\n",
        "State 1 has a value of approximately 99.9997\n",
        "\n",
        "State 2 has a value of approximately 79.9997\n",
        "\n",
        "State 3 has a value of 0"
      ],
      "metadata": {
        "id": "B8_xVUlb6MOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Clarification"
      ],
      "metadata": {
        "id": "2MWK8j7C7PSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These values represent the expected cumulative reward from each state if the agent follows the optimal policy.\n",
        "\n",
        "States 0 and 1 have high values (~100), indicating that they are favorable starting points due to potential future rewards.\n",
        "\n",
        "State 2 has a slightly lower value (~80), implying it has access to rewards but not as favorable as States 0 and 1.\n",
        "\n",
        "State 3 has a value of 0, possibly representing a terminal state where no further rewards can be obtained."
      ],
      "metadata": {
        "id": "7A24PEWj6mWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "wF-Yx2hG6ulv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus the policy evaluation and policy improvement for given MDP has been performed successfully. When we are not sure about the outcome (uncertainity), MDP is very useful. Initially defined all the necessary stuffs for MDP. Policy Evaluation and Policy Improvement function has been implemented. At last got the optimal policy and optimal value."
      ],
      "metadata": {
        "id": "b3mdamnQ6wHG"
      }
    }
  ]
}