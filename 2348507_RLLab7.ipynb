{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imBTQsiOxYl2",
    "outputId": "0d3de9f2-1bb1-4235-f975-00e313d746ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned State-Value Function:\n",
      "[[-9.26 -9.1  -8.73 -8.42]\n",
      " [-9.09 -8.73 -8.44 -7.83]\n",
      " [-8.76 -8.16 -6.49 -4.78]\n",
      " [-8.23 -6.75 -3.79  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Grid World environment\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size):\n",
    "        self.grid_size = grid_size\n",
    "        self.state = (0, 0)  # Start state (top-left)\n",
    "        self.terminal_state = (grid_size - 1, grid_size - 1)\n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 'up':\n",
    "            x = max(x - 1, 0)\n",
    "        elif action == 'down':\n",
    "            x = min(x + 1, self.grid_size - 1)\n",
    "        elif action == 'left':\n",
    "            y = max(y - 1, 0)\n",
    "        elif action == 'right':\n",
    "            y = min(y + 1, self.grid_size - 1)\n",
    "\n",
    "        self.state = (x, y)\n",
    "        reward = -1 if self.state != self.terminal_state else 0\n",
    "        done = self.state == self.terminal_state\n",
    "        return self.state, reward, done\n",
    "\n",
    "def td_learning(env, episodes, alpha, gamma):\n",
    "    grid_size = env.grid_size\n",
    "    state_values = np.zeros((grid_size, grid_size))  # Initialize state-value function\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.choice(env.actions)\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            # Update state-value function using TD(0)\n",
    "            x, y = state\n",
    "            nx, ny = next_state\n",
    "            td_target = reward + gamma * state_values[nx, ny]  # Bootstrapping with next state's value\n",
    "            td_error = td_target - state_values[x, y]\n",
    "            state_values[x, y] += alpha * td_error\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return state_values\n",
    "\n",
    "# Parameters\n",
    "grid_size = 4\n",
    "episodes = 1000\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "env = GridWorld(grid_size)\n",
    "state_values = td_learning(env, episodes, alpha, gamma)\n",
    "\n",
    "print(\"Learned State-Value Function:\")\n",
    "print(np.round(state_values, 2))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
