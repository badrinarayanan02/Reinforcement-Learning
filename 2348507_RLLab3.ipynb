{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlOKgdFWXeCPwT1ev2hLFj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrinarayanan02/Reinforcement-Learning/blob/main/2348507_RLLab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Markov Decision Process (MDP) Simulation and Value Iteration"
      ],
      "metadata": {
        "id": "JLJAD5cFbCJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MDP** is a mathematical framework that is used to make decisions on uncertainity. It maximizes the expected future reward. We need to define states, actions, transitions, rewards, discount."
      ],
      "metadata": {
        "id": "7EJR5RJqsAfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performing Value Iteration for a given MDP"
      ],
      "metadata": {
        "id": "FCAhoMCfbie8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Value Iteration algorithm iteratively calculates the optimal state-value function by updating values based on maximum expected future rewards, converging to an optimal policy. It is a dynamic programming algorithm."
      ],
      "metadata": {
        "id": "T_XpwuxCbrrL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Libraries"
      ],
      "metadata": {
        "id": "qJdR7qXrcDog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "TkXESbWsbnKL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CB-rxZUXcIX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MDP Parameters"
      ],
      "metadata": {
        "id": "r_VTNZrncIZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_states = 4\n",
        "num_actions = 2\n",
        "\n",
        "P = {\n",
        "    0: {\n",
        "        0: [(1.0, 1, 10)],\n",
        "        1: [(1.0, 0, 0)]\n",
        "    },\n",
        "    1: {\n",
        "        0: [(1.0, 2, -10)],\n",
        "        1: [(1.0, 0, 10)]\n",
        "    },\n",
        "    2: {\n",
        "        0: [(1.0, 3, 10)],\n",
        "        1: [(1.0, 1, -10)]\n",
        "    },\n",
        "    3: {\n",
        "        0: [(1.0, 3, 0)],\n",
        "        1: [(1.0, 3, 0)]\n",
        "    }\n",
        "}\n",
        "\n",
        "gamma = 0.9  # Discount factor\n",
        "theta = 1e-4 # Convergence threshold"
      ],
      "metadata": {
        "id": "ZyR3SHYUb1tQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "eRz_P0tYcROa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented states and actions in MDP process with specified transitions and rewards and initializes the parameters and an initial policy for policy iteration. Specified parameters are Gamma and Theta. Gamma is a discount factor, the value of 0.9 means the future rewards are discounted by 10% for each time setup. Theta value of 1e-4 ensures that the process will stop when the value updates are less than this threshold."
      ],
      "metadata": {
        "id": "g127nPNUcRPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Value Iteration"
      ],
      "metadata": {
        "id": "JL69byrIcgc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(P, gamma, theta):\n",
        "    V = np.zeros(num_states)\n",
        "    policy = np.zeros(num_states, dtype=int)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(num_states):\n",
        "            # Calculating the value for each action\n",
        "            action_values = np.zeros(num_actions)\n",
        "            for a in range(num_actions):\n",
        "                action_value = 0\n",
        "                for prob, next_state, reward in P[s][a]:\n",
        "                    action_value += prob * (reward + gamma * V[next_state])\n",
        "                action_values[a] = action_value\n",
        "\n",
        "            max_value = np.max(action_values)\n",
        "            delta = max(delta, abs(max_value - V[s]))\n",
        "            V[s] = max_value\n",
        "            policy[s] = np.argmax(action_values)  # Storing the best action in the policy\n",
        "\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "UeG8gFpHcSBj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "Engg5GDDd5G_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function finds the best way to find optimal policy in each state to maximize rewards. It does this by repeatedly calculating the potential future rewards for each action, choosing the best action for each state, and updating values until they no longer change significantly."
      ],
      "metadata": {
        "id": "tbemZd1Sd6kS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimal Policy and Optimal Value"
      ],
      "metadata": {
        "id": "yWzEJgHLdQ9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy, optimal_value = value_iteration(P, gamma, theta)\n",
        "\n",
        "print(\"Optimal Policy (state-action pairs):\")\n",
        "print(optimal_policy)\n",
        "print(\"\\nOptimal State-Value Function:\")\n",
        "print(optimal_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT7wpS5PctCR",
        "outputId": "269cd600-5125-4f29-aaac-4b5c97ac7703"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy (state-action pairs):\n",
            "[0 1 1 0]\n",
            "\n",
            "Optimal State-Value Function:\n",
            "[99.99964119 99.99967708 79.99970937  0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "uR4zE3Z6c5Yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Optimal Policy, each element corresponds to the best action for each state. The best action from state 0 is action 0, from state 1 is action 1 etc. Optimal State Value Function shows the maximum expected reward for each state under the optimal policy."
      ],
      "metadata": {
        "id": "llDoxdY2c6mv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "0AUrg3JoeXI0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus the value iteration for the markov decision process has been implemented successfully. When we are not sure about the outcome (uncertainity), MDP is very useful. Initially defined all the necessary stuffs for MDP. Value Iteration function has been implemented. At last got the optimal policy and optimal value."
      ],
      "metadata": {
        "id": "m9D06qXeeYui"
      }
    }
  ]
}