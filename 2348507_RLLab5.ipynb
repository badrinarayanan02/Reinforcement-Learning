{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIzdkQUMj4WZ",
    "outputId": "859505cd-3484-4db9-d7d8-be84ea413cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (State -> Best Action):\n",
      "State (0, 0): Action 1\n",
      "State (0, 1): Action 1\n",
      "State (0, 2): Action 1\n",
      "State (0, 3): Action 2\n",
      "State (1, 0): Action 0\n",
      "State (1, 1): Action 0\n",
      "State (1, 2): Action 0\n",
      "State (1, 3): Action 2\n",
      "State (2, 0): Action 1\n",
      "State (2, 1): Action 2\n",
      "State (2, 2): Action 3\n",
      "State (2, 3): Action 2\n",
      "State (3, 0): Action 1\n",
      "State (3, 1): Action 1\n",
      "State (3, 2): Action 1\n",
      "State (3, 3): Action 0\n",
      ". . . .\n",
      ". . . .\n",
      ". . . .\n",
      ". . . A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size=4, terminal_states=None, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Simple grid world environment.\n",
    "        :param grid_size: Size of the grid (grid_size x grid_size).\n",
    "        :param terminal_states: List of terminal states as (row, col).\n",
    "        :param gamma: Discount factor.\n",
    "        \"\"\"\n",
    "        self.grid_size = grid_size\n",
    "        self.terminal_states = terminal_states or [(grid_size - 1, grid_size - 1)]\n",
    "        self.gamma = gamma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.agent_pos = (0, 0)  # Start at top-left corner\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a step in the environment.\n",
    "        :param action: Action to take (0=Up, 1=Right, 2=Down, 3=Left).\n",
    "        :return: next_state, reward, done.\n",
    "        \"\"\"\n",
    "        row, col = self.agent_pos\n",
    "        moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}  # Up, Right, Down, Left\n",
    "        dr, dc = moves[action]\n",
    "        new_row = max(0, min(self.grid_size - 1, row + dr))\n",
    "        new_col = max(0, min(self.grid_size - 1, col + dc))\n",
    "        self.agent_pos = (new_row, new_col)\n",
    "\n",
    "        if self.agent_pos in self.terminal_states:\n",
    "            return self.agent_pos, 10, True  # Reward +10 for reaching terminal state\n",
    "        return self.agent_pos, -1, False  # Step penalty -1\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Print the current state of the grid.\"\"\"\n",
    "        grid = [['.' for _ in range(self.grid_size)] for _ in range(self.grid_size)]\n",
    "        for r, c in self.terminal_states:\n",
    "            grid[r][c] = 'T'  # Mark terminal states\n",
    "        row, col = self.agent_pos\n",
    "        grid[row][col] = 'A'  # Mark agent position\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "\n",
    "class MonteCarloControl:\n",
    "    def __init__(self, env, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Monte Carlo Control algorithm implementation.\n",
    "        :param env: The environment object.\n",
    "        :param gamma: Discount factor.\n",
    "        :param epsilon: Exploration rate.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = {}  # Action-value function Q(s, a)\n",
    "        self.returns = {}  # Returns for state-action pairs\n",
    "        self.policy = {}  # Policy Ï€(s)\n",
    "\n",
    "        for row in range(env.grid_size):\n",
    "            for col in range(env.grid_size):\n",
    "                state = (row, col)\n",
    "                self.Q[state] = [0, 0, 0, 0]  # Initialize Q-values for all actions\n",
    "                self.returns[state] = {a: [] for a in range(4)}  # Initialize returns\n",
    "                self.policy[state] = [0.25, 0.25, 0.25, 0.25]  # Epsilon-greedy policy\n",
    "\n",
    "    def epsilon_greedy(self, state):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(range(4))  # Explore: random action\n",
    "        return self.Q[state].index(max(self.Q[state]))  # Exploit: best action\n",
    "\n",
    "    def generate_episode(self):\n",
    "        \"\"\"Generate an episode using the current policy.\"\"\"\n",
    "        episode = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.epsilon_greedy(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        return episode\n",
    "\n",
    "    def optimize_policy(self, num_episodes=1000):\n",
    "        \"\"\"Run Monte Carlo Control to optimize the policy.\"\"\"\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "\n",
    "            # Compute returns for the episode\n",
    "            G = 0  # Initialize return\n",
    "            visited = set()  # Track visited state-action pairs in this episode\n",
    "            for state, action, reward in reversed(episode):\n",
    "                G = reward + self.gamma * G\n",
    "                if (state, action) not in visited:\n",
    "                    visited.add((state, action))\n",
    "                    self.returns[state][action].append(G)\n",
    "                    self.Q[state][action] = sum(self.returns[state][action]) / len(self.returns[state][action])\n",
    "\n",
    "            # Update the policy\n",
    "            for state in self.policy:\n",
    "                best_action = self.Q[state].index(max(self.Q[state]))\n",
    "                for action in range(4):\n",
    "                    if action == best_action:\n",
    "                        self.policy[state][action] = 1 - self.epsilon + (self.epsilon / 4)\n",
    "                    else:\n",
    "                        self.policy[state][action] = self.epsilon / 4\n",
    "        return self.policy\n",
    "\n",
    "env = GridWorld(grid_size=4, terminal_states=[(3, 3)])\n",
    "mc_control = MonteCarloControl(env)\n",
    "\n",
    "policy = mc_control.optimize_policy(num_episodes=1000)\n",
    "\n",
    "print(\"Optimal Policy (State -> Best Action):\")\n",
    "for state in sorted(mc_control.policy):\n",
    "    best_action = mc_control.Q[state].index(max(mc_control.Q[state]))\n",
    "    print(f\"State {state}: Action {best_action}\")\n",
    "\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
