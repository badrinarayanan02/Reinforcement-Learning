{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v95a8t6vQDl"
   },
   "source": [
    "### Key Components of the Code\n",
    "\n",
    "1. **Environment Initialization**\n",
    "The gird size and terminal stated are configurable:\n",
    "\n",
    "```bash\n",
    "env = ModelBasedGridWorld(grid_size=5, terminal_states=[(4,4)])\n",
    "```\n",
    "\n",
    "- `grid_size=5`: Create a 5x5 grid.\n",
    "- `terminal_states=[(4, 4)]`:  Sets the bottom-right corner as the goal (terminal state).\n",
    "- The agent starts at the top-left corner `(0, 0)`.\n",
    "\n",
    "2. **Actions**\n",
    "The agent has 4 discrete actions:\n",
    "\n",
    "- **Up (0)**: Move up by one cell.\n",
    "- **Right (1)**: Move right by one cell.\n",
    "- **Down (2)**: Move down by one cell.\n",
    "- **Left (3)**: Move left by one cell.\n",
    "\n",
    "Each action transitions the agent to a new state, unless:\n",
    "- It would move the agent out of bounds (e.g., moving up from the top row does nothing).\n",
    "\n",
    "3. **State Transition**\n",
    "The function step(action) defines how the agent moves within the grid:\n",
    "\n",
    "```bash\n",
    "# Movement definitions: {0: Up, 1: Right, 2: Down, 3: Left}\n",
    "moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}\n",
    "```\n",
    "\n",
    "- Each action modifies the agent's row and column coordinates.\n",
    "- Boundary conditions ensure the agent doesn’t move outside the grid:\n",
    "\n",
    "```bash\n",
    "next_row = max(0, min(self.grid_size - 1, next_row))\n",
    "next_col = max(0, min(self.grid_size - 1, next_col))\n",
    "```\n",
    "\n",
    "4. **Rewards**\n",
    "Rewards guide the agent’s behavior:\n",
    "- Reaching a terminal state (goal) yields a positive reward (e.g., +10).\n",
    "- Each step taken without reaching the goal incurs a negative reward (e.g., -1).\n",
    "This reward structure incentivizes the agent to reach the terminal state in as few steps as possible.\n",
    "\n",
    "5. **Terminal States**\n",
    "The episode ends when the agent reaches one of the **terminal states**:\n",
    "\n",
    "```bash\n",
    "done = next_state in self.terminal_states\n",
    "```\n",
    "\n",
    "6. **Rendering**\n",
    "The `render()` method visualizes the current state of the grid:\n",
    "\n",
    "The grid displays:\n",
    "- `A`: The agent’s current position.\n",
    "- `T`: Terminal states (goals).\n",
    "- `.`: Empty cells.\n",
    "\n",
    "Example visualization:\n",
    "```bash\n",
    "A . . . .\n",
    ". . . . .\n",
    ". . . . .\n",
    ". . . . .\n",
    ". . . . T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hOWAjA8kLq-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p9fqv9XvVih"
   },
   "outputs": [],
   "source": [
    "class ModelBasedGridWorld(gym.Env):\n",
    "    def __init__(self, grid_size=5, terminal_states=None, random_seed=None):\n",
    "        \"\"\"\n",
    "        Custom GridWorld environment for model-based reinforcement learning.\n",
    "        :param grid_size: Size of the grid (grid_size x grid_size).\n",
    "        :param terminal_states: List of terminal state positions (row, col).\n",
    "        :param random_seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.terminal_states = terminal_states or [(grid_size - 1, grid_size - 1)]\n",
    "        self.random_seed = random_seed\n",
    "        self._setup_environment()\n",
    "\n",
    "    def _setup_environment(self):\n",
    "        # Initialize state and action spaces\n",
    "        self.action_space = spaces.Discrete(4)  # Actions: 0=Up, 1=Right, 2=Down, 3=Left\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_size, self.grid_size])\n",
    "        self.state = (0, 0)  # Start at top-left corner\n",
    "        self.reward_model = {}  # Reward function R(s, a)\n",
    "        self.transition_model = {}  # Transition dynamics P(s'|s, a)\n",
    "        if self.random_seed:\n",
    "            np.random.seed(self.random_seed)\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return the next state, reward, done, and info.\"\"\"\n",
    "        if action < 0 or action >= self.action_space.n:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "\n",
    "        row, col = self.state\n",
    "        if self.state in self.terminal_states:\n",
    "            return self.state, 0, True, {}\n",
    "\n",
    "        moves = {0: (-1, 0), 1: (0, 1), 2: (1, 0), 3: (0, -1)}  # Up, Right, Down, Left\n",
    "        dr, dc = moves[action]\n",
    "        next_row, next_col = row + dr, col + dc\n",
    "\n",
    "        # Ensure the next state stays within bounds\n",
    "        next_row = max(0, min(self.grid_size - 1, next_row))\n",
    "        next_col = max(0, min(self.grid_size - 1, next_col))\n",
    "        next_state = (next_row, next_col)\n",
    "\n",
    "        # Reward is -1 for each step unless in terminal state\n",
    "        reward = -1 if next_state not in self.terminal_states else 10\n",
    "        done = next_state in self.terminal_states\n",
    "\n",
    "        self.state = next_state\n",
    "\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to the initial state.\"\"\"\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the current state of the environment.\"\"\"\n",
    "        grid = np.zeros((self.grid_size, self.grid_size), dtype=str)\n",
    "        grid[:, :] = '.'\n",
    "        for r, c in self.terminal_states:\n",
    "            grid[r, c] = 'T'  # Mark terminal states\n",
    "        row, col = self.state\n",
    "        grid[row, col] = 'A'  # Mark agent position\n",
    "        print(\"\\n\".join([\" \".join(row) for row in grid]))\n",
    "        print()\n",
    "\n",
    "    def get_transition_model(self):\n",
    "        \"\"\"Return the transition dynamics for model-based RL.\"\"\"\n",
    "        return self.transition_model\n",
    "\n",
    "    def get_reward_model(self):\n",
    "        \"\"\"Return the reward function for model-based RL.\"\"\"\n",
    "        return self.reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gD1n7Z0DvVkp",
    "outputId": "a5cf0d51-8cfd-47d0-f0d0-96486d2ac685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 0, Reward: -1\n",
      ". A . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 1, Reward: -1\n",
      ". . . . .\n",
      ". A . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 2, Reward: -1\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 3, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 2, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 3, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      "A . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 2, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". A . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 1, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . A . .\n",
      ". . . . T\n",
      "\n",
      "Action: 1, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . A . .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 0, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . A .\n",
      ". . . . .\n",
      ". . . . T\n",
      "\n",
      "Action: 1, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . A .\n",
      ". . . . T\n",
      "\n",
      "Action: 2, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . A T\n",
      "\n",
      "Action: 2, Reward: -1\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . .\n",
      ". . . . A\n",
      "\n",
      "Action: 1, Reward: 10\n"
     ]
    }
   ],
   "source": [
    "env = ModelBasedGridWorld(grid_size=5, terminal_states=[(4, 4)])\n",
    "\n",
    "state = env.reset()\n",
    "env.render()\n",
    "\n",
    "# Simulate a random episode\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Action: {action}, Reward: {reward}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
