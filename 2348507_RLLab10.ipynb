{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywMeOBlHF-wV",
        "outputId": "37636c11-6799-49a5-90ff-846f8b1a4d14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Total Reward = 14.0\n",
            "Episode 2: Total Reward = 28.0\n",
            "Episode 3: Total Reward = 40.0\n",
            "Episode 4: Total Reward = 31.0\n",
            "Episode 5: Total Reward = 104.0\n",
            "Episode 6: Total Reward = 17.0\n",
            "Episode 7: Total Reward = 31.0\n",
            "Episode 8: Total Reward = 30.0\n",
            "Episode 9: Total Reward = 55.0\n",
            "Episode 10: Total Reward = 18.0\n",
            "Episode 11: Total Reward = 76.0\n",
            "Episode 12: Total Reward = 29.0\n",
            "Episode 13: Total Reward = 32.0\n",
            "Episode 14: Total Reward = 128.0\n",
            "Episode 15: Total Reward = 133.0\n",
            "Episode 16: Total Reward = 82.0\n",
            "Episode 17: Total Reward = 84.0\n",
            "Episode 18: Total Reward = 72.0\n",
            "Episode 19: Total Reward = 26.0\n",
            "Episode 20: Total Reward = 42.0\n",
            "Episode 21: Total Reward = 78.0\n",
            "Episode 22: Total Reward = 98.0\n",
            "Episode 23: Total Reward = 143.0\n",
            "Episode 24: Total Reward = 121.0\n",
            "Episode 25: Total Reward = 66.0\n",
            "Episode 26: Total Reward = 97.0\n",
            "Episode 27: Total Reward = 85.0\n",
            "Episode 28: Total Reward = 91.0\n",
            "Episode 29: Total Reward = 74.0\n",
            "Episode 30: Total Reward = 55.0\n",
            "Episode 31: Total Reward = 157.0\n",
            "Episode 32: Total Reward = 66.0\n",
            "Episode 33: Total Reward = 102.0\n",
            "Episode 34: Total Reward = 46.0\n",
            "Episode 35: Total Reward = 60.0\n",
            "Episode 36: Total Reward = 58.0\n",
            "Episode 37: Total Reward = 33.0\n",
            "Episode 38: Total Reward = 59.0\n",
            "Episode 39: Total Reward = 176.0\n",
            "Episode 40: Total Reward = 57.0\n",
            "Episode 41: Total Reward = 41.0\n",
            "Episode 42: Total Reward = 32.0\n",
            "Episode 43: Total Reward = 65.0\n",
            "Episode 44: Total Reward = 59.0\n",
            "Episode 45: Total Reward = 162.0\n",
            "Episode 46: Total Reward = 95.0\n",
            "Episode 47: Total Reward = 147.0\n",
            "Episode 48: Total Reward = 72.0\n",
            "Episode 49: Total Reward = 62.0\n",
            "Episode 50: Total Reward = 157.0\n",
            "Episode 51: Total Reward = 221.0\n",
            "Solved the environment!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.softmax(self.fc2(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "def compute_discounted_rewards(rewards, gamma):\n",
        "    discounted_rewards = []\n",
        "    cumulative_reward = 0\n",
        "    for reward in reversed(rewards):\n",
        "        cumulative_reward = reward + gamma * cumulative_reward\n",
        "        discounted_rewards.insert(0, cumulative_reward)\n",
        "    return discounted_rewards\n",
        "\n",
        "def main():\n",
        "    env = gym.make(\"CartPole-v1\")\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "    optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "\n",
        "    num_episodes = 1000\n",
        "    gamma = 0.99  \n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        states, actions, rewards = [], [], []\n",
        "        \n",
        "        done = False\n",
        "        while not done:\n",
        "            state = torch.tensor(state, dtype=torch.float32)\n",
        "            probs = policy_net(state)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "            \n",
        "        discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
        "        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n",
        "\n",
        "        \n",
        "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        for log_prob, reward in zip(states, discounted_rewards):\n",
        "            action_prob = policy_net(log_prob)\n",
        "            action_log_prob = torch.log(action_prob[actions.pop(0)])\n",
        "            loss -= action_log_prob * reward\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_reward = sum(rewards)\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "        if total_reward >= 200:\n",
        "            print(\"Solved the environment!\")\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
